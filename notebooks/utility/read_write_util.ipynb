{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab144483-4fbc-4067-977e-01c66c2aadc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4641e438-e1cb-4369-a60a-1d00432abcc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def file_copy(\n",
    "    source_file_path:str,\n",
    "    target_file_path:str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Copies a file from the specified source path to the target path.\n",
    "\n",
    "    Args:\n",
    "        source_file_path (str): The path to the source file to be copied.\n",
    "        target_file_path (str): The destination path where the file will be copied.\n",
    "    \n",
    "    \"\"\"\n",
    "    dbutils.fs.cp( source_file_path , target_file_path )                                    \n",
    "    print(f\"File Copied from {source_file_path } to : {target_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e303c918-10a0-4715-8c6a-93739b42b673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_data(\n",
    "    source_file_path:str,\n",
    "    source_file_format:str,\n",
    "    read_options:dict={\n",
    "        \"header\": \"true\",\n",
    "        \"inferSchema\": \"true\"\n",
    "    },\n",
    "    lower_case:bool=True,\n",
    "    audit_columns:bool=True):\n",
    "    \"\"\"\n",
    "    Reads data from the specified source file path and loads it into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        source_file_path (str): The path to the source data file.\n",
    "        source_file_format (str): The file format of source data to read (e.g., 'parquet','csv').\n",
    "        read_options (dict): Dictionary of read options to apply when loading the data. Defaults header & inferSchema to True\n",
    "        lower_case (bool, optional): If True, converts column names to lower case. Defaults to True.\n",
    "        audit_columns (bool, optional): If True, adds audit columns such as source filename and insert timestamp. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"============== Reading from {source_file_path} ==============\")\n",
    "\n",
    "    if source_file_format == 'parquet':\n",
    "        df = spark.read.options(**read_options).parquet(source_file_path)\n",
    "\n",
    "    elif source_file_format == 'csv':\n",
    "        df = spark.read.options(**read_options).csv(source_file_path)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Not a valid or supported file format\")\n",
    "\n",
    "    if lower_case:\n",
    "        df = df.selectExpr([ f\"`{c}` as `{c.lower()}`\" for c in df.columns ])\n",
    "    \n",
    "    if audit_columns:\n",
    "        df = df.withColumn('source_filename',f.expr('_metadata.file_path'))\\\n",
    "               .withColumn('insert_timestamp',f.current_timestamp())\n",
    "\n",
    "    print(f\"============== Read from {source_file_path} ==============\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f4e00d-a461-4530-ab78-2abd33d2a79f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_data(\n",
    "    df,\n",
    "    target_file_format:str,\n",
    "    target_file_path:str,\n",
    "    mode_type:str,\n",
    "    write_options:dict={\n",
    "        \"mode\": \"overwrite\"\n",
    "    }\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Writes data from a Spark DataFrame to the specified target file path.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Spark DataFrame containing the data to be written.\n",
    "        target_file_path (str): The path to the target file where the data will be written.\n",
    "        target_file_format (str) : The file format of target data to write (e.g., 'parquet','csv').\n",
    "        mode_type (str) : The mode of writing the data. Defaults to overwrite.\n",
    "        write_options (dict): Dictionary of write options to apply when writing the data. Defaults to mode overwrite.\n",
    "    \"\"\"\n",
    "    print(f\"============== Writing to {target_file_path} ==============\")\n",
    "\n",
    "    if target_file_format == 'delta':\n",
    "        df.write.format(target_file_format).mode(mode_type).options(**write_options).saveAsTable(target_file_path)\n",
    "\n",
    "    else:\n",
    "        df.write.format(target_file_format).mode(mode_type).options(**write_options).save(target_file_path)\n",
    "    \n",
    "    print(f\"============== Wrote to {target_file_path} ==============\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "read_write_util",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
