# ðŸ—½ NYC Taxi Data Engineering Project â€“ Databricks

This project is a **data engineering pipeline** for processing and managing **NYC Taxi trip data** using **Apache Spark on Databricks**. It includes building a scalable ETL pipeline, data quality checks, and performance optimizations with **Delta Lake** for downstream analytics.
It generates insights about taxi usage patterns, trip durations, fares, pickup/dropoff hotspots, and other factors affecting urban mobility
---

## ðŸ§± Project Goals

* Build robust, reusable data ingestion and transformation pipelines
* Use Delta Lake for scalable, ACID-compliant data storage
* Implement data quality checks and handle corrupt records
* Partition and optimize datasets for query performance
* Enable downstream analytics and reporting

---

## ðŸ”§ Tech Stack

| Component       | Technology                        |
| --------------- | --------------------------------- |
| Platform        | Databricks (Community/Enterprise) |
| Language        | Python (PySpark)                  |
| Data Processing | Apache Spark                      |
| Storage Format  | Delta Lake                        |
| Scheduling      | Databricks Workflows / Jobs       |
| Orchestration   | Optional: Airflow / dbx           |
| Data Source     | NYC TLC Trip Records              |

---

## ðŸ“ Project Structure

```
nyc-taxi-data-engineering/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Bronze Layer/
|       â””â”€â”€ DataIngest
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ 
â”‚
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ databricks_job_config.json
â”‚
â”œâ”€â”€ data/ 
â”‚
â”œâ”€â”€ README.md
```

---

## ðŸ› ï¸ Pipeline Architecture

```
            +-------------+
            |  Raw Data   | (CSV/Parquet from TLC)
            +-------------+
                   |
                   â–¼
         +-------------------+
         | Bronze Layer (Raw)|
         | - Basic ingestion |
         +-------------------+
                   |
                   â–¼
        +------------------------+
        | Silver Layer (Cleaned) |
        | - Null handling        |
        | - Schema enforcement   |
        | - Type casting         |
        +------------------------+
                   |
                   â–¼
     +----------------------------------+
     | Gold Layer (Aggregated / Final) |
     | - Summaries, KPIs, Aggregations |
     +----------------------------------+
```

---

## ðŸš€ Key Features

* âœ… Load TLC data from cloud storage or Databricks datasets
* âœ… Use **Auto Loader** for incremental file ingestion (optional)
* âœ… Schema inference with validation against expected types
* âœ… Write in **Delta Lake** format with partitioning (e.g., by year/month)
* âœ… Handle bad/corrupt records and log errors
* âœ… Perform aggregations for downstream BI/analytics use
* âœ… Schedule with Databricks Jobs or Workflows

---

## ðŸ’¡ Example Transformations

* Convert pickup/dropoff timestamps to datetime
* Compute trip duration in minutes
* Filter out invalid coordinates and zero-distance trips
* Generate daily and monthly aggregates by zone
* Optimize tables with `ZORDER` and `OPTIMIZE`

---

## ðŸ“¥ Sample Data Source

NYC TLC Public Dataset (Yellow Taxi):

```python
df = spark.read.format("parquet").load("/databricks-datasets/nyctaxi/tripdata/yellow/")
```

Or load from S3/ADLS:

```python
df = spark.read.csv("s3://my-bucket/nyc-taxi/yellow_tripdata_*.csv", header=True)
```

---

## âœ… Data Quality Checks

* Null value checks (pickup/dropoff datetime, fare, etc.)
* Range validation (e.g., positive fare amount, duration > 0)
* Schema mismatch alerts
* Duplicate row detection (based on `trip_id` or composite keys)

---

## ðŸ“† Job Orchestration

Use **Databricks Workflows** or **Databricks Jobs** for orchestration. Example:

* Run ingestion daily or hourly
* Trigger cleaning after ingestion
* Schedule aggregation jobs weekly

---

## ðŸ“Š Outputs

* Delta Tables for each layer: `/delta/bronze/`, `/delta/silver/`, `/delta/gold/`
* Final gold tables ready for analytics in Databricks SQL or Power BI/Tableau
* Logs & audit records of corrupt/malformed data

---

## ðŸ”® Future Enhancements

* Add **MLflow** tracking for predictions (e.g., fare estimator)
* Integrate weather or traffic datasets
* Real-time streaming ingestion with **Auto Loader + Structured Streaming**
* Deployment using **dbx CLI** for CI/CD

---

## ðŸ‘¤ Author

*Vamsi Krishna*
\[LinkedIn / GitHub / Portfolio]

---

## ðŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

